# ğŸŒ Web Content Extraction API ğŸ“„

## ğŸ”¥ Votre passerelle unifiÃ©e vers l'extraction intelligente de contenu web ğŸ”¥

[![GitHub stars](https://img.shields.io/github/stars/simonpierreboucher0/web-content-api?style=social)](https://github.com/simonpierreboucher0/web-content-api/stargazers)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![Python 3.8+](https://img.shields.io/badge/python-3.8+-blue.svg)](https://www.python.org/downloads/)
[![FastAPI](https://img.shields.io/badge/FastAPI-0.105.0+-green.svg)](https://fastapi.tiangolo.com/)

---

## âœ¨ CaractÃ©ristiques principales

ğŸ“„ **Extraction puissante** - RÃ©cupÃ©rez le contenu brut et traitÃ© des pages web  
ğŸ”„ **Interface unifiÃ©e** - Une seule API pour diffÃ©rents moteurs d'extraction  
ğŸ§© **Normalisation des donnÃ©es** - Format standardisÃ© quelle que soit la source  
âš¡ **Haute performance** - Mise en cache intelligente des rÃ©sultats  
ğŸ›¡ï¸ **Robustesse intÃ©grÃ©e** - Gestion avancÃ©e des erreurs et des timeouts  
ğŸ“Š **Logging dÃ©taillÃ©** - Suivi complet des opÃ©rations d'extraction  
ğŸŒ **Support multilingue** - Extraction du contenu en plusieurs langues  
ğŸ”Œ **Failover automatique** - Bascule intelligente entre les fournisseurs  

---

## ğŸ¤– Moteurs d'extraction pris en charge

### ğŸŸ£ Exa AI
Extraction sophistiquÃ©e avec analyse sÃ©mantique avancÃ©e.

| FonctionnalitÃ© | Description | Avantage |
|----------------|-------------|----------|
| ğŸ“„ **Extraction de texte** | RÃ©cupÃ©ration du contenu principal | Ã‰limination du bruit et publicitÃ©s |
| ğŸ–¼ï¸ **RÃ©cupÃ©ration d'images** | Extraction des images pertinentes | Contenu visuel associÃ© |
| ğŸ”— **Analyse des liens** | Collecte des liens internes et externes | Cartographie complÃ¨te du contenu |
| ğŸ“‹ **MÃ©tadonnÃ©es structurÃ©es** | Extraction de title, author, date... | Information contextuelle enrichie |
| ğŸ“‘ **Sous-pages intelligentes** | Exploration des pages liÃ©es pertinentes | Contenu connexe automatiquement dÃ©tectÃ© |
| ğŸ“Š **Scores de pertinence** | Ã‰valuation de sections importantes | Identification des passages clÃ©s |
| ğŸ” **Mise en Ã©vidence** | Extraction des passages significatifs | Points essentiels rapidement identifiÃ©s |
| ğŸ“ **GÃ©nÃ©ration de rÃ©sumÃ©s** | SynthÃ¨se automatique du contenu | Vue d'ensemble instantanÃ©e |

### ğŸŸ¢ Tavily
Extraction prÃ©cise optimisÃ©e pour la rÃ©cupÃ©ration d'information structurÃ©e.

| FonctionnalitÃ© | Description | Avantage |
|----------------|-------------|----------|
| ğŸ” **Profondeur d'extraction** | Modes basic et advanced | FlexibilitÃ© selon les besoins |
| ğŸ“œ **Contenu brut** | Texte original de la page | DonnÃ©es non filtrÃ©es pour analyse personnalisÃ©e |
| ğŸ“Š **Contenu structurÃ©** | Texte nettoyÃ© et organisÃ© | Facilite le traitement ultÃ©rieur |
| ğŸ—ï¸ **Extraction de structure** | Organisation hiÃ©rarchique du contenu | ComprÃ©hension de l'architecture de la page |
| ğŸ–¼ï¸ **Images contextuelles** | Capture des images avec mÃ©tadonnÃ©es | Enrichissement visuel du contenu |
| ğŸ” **DÃ©tection de contenu dynamique** | Gestion du JavaScript et contenu chargÃ© dynamiquement | Extraction complÃ¨te mÃªme sur sites modernes |
| â±ï¸ **Optimisation des performances** | Extraction rapide et efficace | Traitement de grand volume d'URLs |
| ğŸ§© **CompatibilitÃ© multiformat** | Support de divers formats web | Large couverture de sites |

---

## ğŸ› ï¸ Installation facile en 3 Ã©tapes

### 1ï¸âƒ£ Cloner le dÃ©pÃ´t
```bash
git clone https://github.com/simonpierreboucher0/web-content-api.git
cd web-content-api
```

### 2ï¸âƒ£ Installer les dÃ©pendances
```bash
pip install -r requirements.txt
```

### 3ï¸âƒ£ Configurer vos clÃ©s API
CrÃ©ez un fichier `.env` avec vos clÃ©s:
```ini
TAVILY_KEY=votre_clÃ©_tavily
EXA_KEY=votre_clÃ©_exa
```

> ğŸ’¡ **Astuce**: Vous n'avez besoin de fournir que les clÃ©s pour les moteurs que vous allez utiliser!

---

## ğŸš€ Guide d'utilisation rapide

### â–¶ï¸ DÃ©marrer le serveur
```bash
uvicorn main:app --reload --host 0.0.0.0 --port 8000
```

### ğŸŒ Documentation interactive
AccÃ©dez Ã  l'interface Swagger pour tester l'API:
```
http://localhost:8000/docs
```

### ğŸ“„ Extraction simple avec Tavily
```bash
curl -X 'POST' \
  'http://localhost:8000/extract' \
  -H 'Content-Type: application/json' \
  -d '{
    "urls": "https://en.wikipedia.org/wiki/Artificial_intelligence",
    "engine": "tavily",
    "include_images": true,
    "extract_depth": "basic"
  }'
```

### ğŸ“‘ Extraction avancÃ©e avec Exa
```bash
curl -X 'POST' \
  'http://localhost:8000/extract' \
  -H 'Content-Type: application/json' \
  -d '{
    "urls": "https://en.wikipedia.org/wiki/Artificial_intelligence",
    "engine": "exa",
    "include_images": true,
    "include_links": true
  }'
```

### ğŸ”„ Extraction automatique (meilleur moteur disponible)
```bash
curl -X 'POST' \
  'http://localhost:8000/extract' \
  -H 'Content-Type: application/json' \
  -d '{
    "urls": "https://en.wikipedia.org/wiki/Artificial_intelligence",
    "engine": "auto",
    "include_images": true
  }'
```

### ğŸ“š Extraction de plusieurs URLs
```bash
curl -X 'POST' \
  'http://localhost:8000/extract' \
  -H 'Content-Type: application/json' \
  -d '{
    "urls": [
      "https://en.wikipedia.org/wiki/Artificial_intelligence",
      "https://arxiv.org/abs/2307.06435"
    ],
    "engine": "exa",
    "include_links": true
  }'
```

---

## ğŸ“Š Structure complÃ¨te des requÃªtes

```json
{
  "urls": "https://example.com/page",            // ğŸ”— URL ou liste d'URLs (obligatoire)
  "engine": "exa",                               // ğŸŒ Moteur d'extraction (exa, tavily, auto)
  "include_images": true,                        // ğŸ–¼ï¸ Inclure les images
  "include_raw_content": false,                  // ğŸ“œ Inclure le contenu brut non traitÃ©
  "extract_depth": "advanced",                   // ğŸ” Profondeur d'extraction (basic, advanced)
  "include_links": true,                         // ğŸ”— Inclure les liens trouvÃ©s
  "max_tokens": 10000                            // ğŸ“ Limite de taille du texte extrait
}
```

---

## ğŸ“‹ Format des rÃ©sultats

```json
{
  "request_id": "550e8400-e29b-41d4-a716-446655440000", // ğŸ†” Identifiant unique de requÃªte
  "urls": ["https://example.com/page"],                // ğŸ”— URLs d'origine
  "results": [                                         // ğŸ“Š Liste des rÃ©sultats
    {
      "url": "https://example.com/page",               // ğŸ”— URL de la page
      "title": "Titre de la page web",                 // ğŸ“ Titre de la page
      "content": "Contenu principal extrait...",       // ğŸ“„ Contenu traitÃ©
      "raw_content": "HTML brut ou texte complet...",  // ğŸ“œ Contenu brut (si demandÃ©)
      "author": "Nom de l'auteur",                     // âœï¸ Auteur si disponible
      "published_date": "2024-01-15T14:30:00Z",        // ğŸ“… Date de publication
      "images": [                                       // ğŸ–¼ï¸ Images extraites
        {
          "url": "https://example.com/image.jpg",      // ğŸ”— URL de l'image
          "alt_text": "Description de l'image",        // ğŸ“ Texte alternatif
          "width": 800,                                // ğŸ“ Largeur en pixels
          "height": 600                                // ğŸ“ Hauteur en pixels
        }
      ],
      "links": [                                       // ğŸ”— Liens extraits
        {
          "url": "https://example.com/related",        // ğŸ”— URL du lien
          "text": "Texte du lien",                     // ğŸ“ Texte d'ancrage
          "is_internal": true                          // ğŸ  Lien interne ou externe
        }
      ],
      "favicon": "https://example.com/favicon.ico",    // ğŸ”– Favicon du site
      "subpages": [                                    // ğŸ“‘ Sous-pages liÃ©es
        {
          "url": "https://example.com/subpage",        // ğŸ”— URL de la sous-page
          "title": "Titre de la sous-page",            // ğŸ“ Titre
          "content": "Extrait de la sous-page..."      // ğŸ“„ Contenu
        }
      ]
    }
  ],
  "failed_urls": [],                                   // âŒ URLs qui ont Ã©chouÃ©
  "engine": "exa",                                     // ğŸŒ Moteur utilisÃ©
  "time_taken": 0.35,                                  // â±ï¸ Temps d'exÃ©cution (secondes)
  "cached": false,                                     // ğŸ”„ RÃ©sultat depuis le cache?
  "status": "ok",                                      // âœ… Ã‰tat de la requÃªte
  "cost_info": {                                       // ğŸ’° Informations de coÃ»t
    "total_urls": 1,                                   // ğŸ”¢ Nombre total d'URLs
    "successful_extractions": 1,                       // âœ… Extractions rÃ©ussies
    "cached_results": 0,                               // ğŸ”„ RÃ©sultats depuis le cache
    "api_calls": 1                                     // ğŸ“ Nombre d'appels API effectuÃ©s
  }
}
```

---

## ğŸ’¡ Cas d'utilisation avancÃ©s

### ğŸ”„ Failover automatique entre moteurs
Si un moteur est indisponible, l'API bascule automatiquement vers le suivant:

```python
import requests

def extract_with_failover(url, engines=["exa", "tavily"]):
    for engine in engines:
        try:
            response = requests.post(
                "http://localhost:8000/extract",
                json={
                    "urls": url,
                    "engine": engine,
                    "include_images": True
                },
                timeout=20
            )
            if response.status_code == 200:
                return response.json()
        except Exception as e:
            print(f"Ã‰chec avec {engine}: {str(e)}")
    return {"error": "Tous les moteurs ont Ã©chouÃ©"}

results = extract_with_failover("https://arxiv.org/abs/2307.06435")
```

### ğŸ“‘ Extraction parallÃ¨le de plusieurs URLs

```python
import requests
import concurrent.futures

def batch_extract(urls, engine="auto"):
    # DÃ©couper en lots de 5 URLs
    batches = [urls[i:i+5] for i in range(0, len(urls), 5)]
    all_results = []
    failed_urls = []
    
    def extract_batch(batch):
        try:
            response = requests.post(
                "http://localhost:8000/extract",
                json={
                    "urls": batch,
                    "engine": engine,
                    "include_images": True,
                    "extract_depth": "basic"
                },
                timeout=30
            )
            if response.status_code == 200:
                data = response.json()
                return data["results"], data["failed_urls"]
            return [], batch
        except Exception as e:
            print(f"Erreur batch: {str(e)}")
            return [], batch
    
    with concurrent.futures.ThreadPoolExecutor(max_workers=3) as executor:
        futures = [executor.submit(extract_batch, batch) for batch in batches]
        
        for future in concurrent.futures.as_completed(futures):
            results, failures = future.result()
            all_results.extend(results)
            failed_urls.extend(failures)
    
    return all_results, failed_urls

# Liste d'URLs Ã  traiter
news_sites = [
    "https://www.reuters.com/technology/ai",
    "https://www.theverge.com/ai-artificial-intelligence",
    "https://www.wired.com/category/artificial-intelligence/",
    "https://venturebeat.com/category/ai/",
    "https://techcrunch.com/category/artificial-intelligence/",
    "https://www.technologyreview.com/topic/artificial-intelligence/",
    "https://www.zdnet.com/topic/artificial-intelligence/"
]

results, failures = batch_extract(news_sites, "exa")
print(f"Extractions rÃ©ussies: {len(results)}, Ã‰checs: {len(failures)}")
```

### ğŸ“Š Comparaison d'extraction entre moteurs

```python
import requests
import concurrent.futures

def compare_extraction_engines(url):
    engines = ["exa", "tavily"]
    results = {}
    
    def extract_with_engine(engine):
        try:
            response = requests.post(
                "http://localhost:8000/extract",
                json={
                    "urls": url, 
                    "engine": engine,
                    "include_images": True,
                    "extract_depth": "advanced" if engine == "tavily" else "basic"
                },
                timeout=25
            )
            if response.status_code == 200:
                return engine, response.json()
            return engine, {"error": f"Status code: {response.status_code}"}
        except Exception as e:
            return engine, {"error": str(e)}
    
    with concurrent.futures.ThreadPoolExecutor() as executor:
        for engine, result in executor.map(extract_with_engine, engines):
            results[engine] = result
    
    return results

comparison = compare_extraction_engines("https://en.wikipedia.org/wiki/Machine_learning")

# Analyse des rÃ©sultats
for engine, data in comparison.items():
    if "error" in data:
        print(f"âš ï¸ {engine}: Erreur - {data['error']}")
        continue
        
    print(f"ğŸ” {engine}: Extraction en {data['time_taken']:.2f}s")
    
    if data["results"]:
        content_length = len(data["results"][0].get("content", ""))
        image_count = len(data["results"][0].get("images", []))
        print(f"  ğŸ“„ Taille du contenu: {content_length} caractÃ¨res")
        print(f"  ğŸ–¼ï¸ Nombre d'images: {image_count}")
        
        if "links" in data["results"][0]:
            link_count = len(data["results"][0]["links"])
            print(f"  ğŸ”— Nombre de liens: {link_count}")
```

### ğŸ“° Extraction d'articles d'actualitÃ©

```python
import requests
import json
from datetime import datetime

def extract_news_article(url):
    response = requests.post(
        "http://localhost:8000/extract",
        json={
            "urls": url,
            "engine": "exa",  # Exa est souvent meilleur pour les articles
            "include_images": True
        }
    )
    
    if response.status_code != 200:
        return {"error": f"Extraction failed: {response.status_code}"}
    
    data = response.json()
    
    if not data["results"]:
        return {"error": "No content extracted"}
    
    article = data["results"][0]
    
    # Formater l'article
    formatted_article = {
        "title": article.get("title", "Untitled Article"),
        "url": article.get("url"),
        "author": article.get("author", "Unknown Author"),
        "published_date": article.get("published_date"),
        "content": article.get("content", "").strip(),
        "word_count": len(article.get("content", "").split()),
        "reading_time_minutes": round(len(article.get("content", "").split()) / 200),  # ~200 mots/minute
        "main_image": article.get("images", [{}])[0].get("url") if article.get("images") else None,
        "extraction_date": datetime.now().isoformat()
    }
    
    return formatted_article

# Exemple
news_url = "https://www.theverge.com/2024/1/15/24039104/openai-anthropic-meta-google-ai-regulation"
article = extract_news_article(news_url)
print(json.dumps(article, indent=2))
```

---

## ğŸ§ª Tests de santÃ© et surveillance

L'API intÃ¨gre un endpoint de santÃ© complet pour surveiller l'Ã©tat des services:

```bash
curl -X GET http://localhost:8000/health
```

Exemple de rÃ©ponse:
```json
{
  "status": "healthy",
  "time": "2024-08-07T14:35:22.451Z",
  "version": "1.0.0",
  "uptime": "2 days, 5:12:30",
  "memory_usage": "98452 KB",
  "hostname": "extraction-api-server",
  "cache_entries": 145,
  "api_statuses": {
    "exa": "ok",
    "tavily": "ok"
  },
  "response_time": "0.0823s"
}
```

---

## ğŸ”’ SÃ©curitÃ© et bonnes pratiques

- ğŸ”‘ **Gestion des clÃ©s**: StockÃ©es localement dans `.env`, jamais exposÃ©es
- ğŸ›¡ï¸ **Rate Limiting**: Protection contre les abus via limitation de taux
- ğŸ”’ **CORS configurable**: Protection contre les requÃªtes non autorisÃ©es
- ğŸ“ **Logging sÃ©curisÃ©**: Masquage automatique des informations sensibles
- â±ï¸ **Timeouts contrÃ´lÃ©s**: Ã‰vite les requÃªtes bloquantes
- ğŸ”„ **Retry avec backoff**: Gestion intelligente des erreurs temporaires
- ğŸ§¹ **Nettoyage des caches**: LibÃ©ration automatique des ressources

---

## ğŸ“š FonctionnalitÃ©s avancÃ©es

### ğŸ“‹ SystÃ¨me de mise en cache
Le systÃ¨me met intelligemment en cache les rÃ©sultats pour optimiser les performances:

```bash
# PremiÃ¨re requÃªte (non mise en cache)
curl -X 'POST' \
  'http://localhost:8000/extract' \
  -H 'Content-Type: application/json' \
  -d '{
    "urls": "https://en.wikipedia.org/wiki/Artificial_intelligence",
    "engine": "tavily",
    "include_images": true
  }'
  
# RequÃªte identique (depuis le cache, beaucoup plus rapide)
curl -X 'POST' \
  'http://localhost:8000/extract' \
  -H 'Content-Type: application/json' \
  -d '{
    "urls": "https://en.wikipedia.org/wiki/Artificial_intelligence",
    "engine": "tavily",
    "include_images": true
  }'
```

### ğŸ“„ Extraction avec diffÃ©rentes profondeurs
Adaptez la profondeur d'extraction selon vos besoins:

```bash
# Extraction basique (plus rapide)
curl -X 'POST' \
  'http://localhost:8000/extract' \
  -H 'Content-Type: application/json' \
  -d '{
    "urls": "https://example.com/article",
    "engine": "tavily",
    "extract_depth": "basic"
  }'

# Extraction avancÃ©e (plus complÃ¨te)
curl -X 'POST' \
  'http://localhost:8000/extract' \
  -H 'Content-Type: application/json' \
  -d '{
    "urls": "https://example.com/article",
    "engine": "tavily",
    "extract_depth": "advanced"
  }'
```

### ğŸ”— Extraction de contenu avec liens et sous-pages
Obtenez une analyse plus complÃ¨te d'une page et de ses liens:

```bash
curl -X 'POST' \
  'http://localhost:8000/extract' \
  -H 'Content-Type: application/json' \
  -d '{
    "urls": "https://docs.python.org/3/tutorial/",
    "engine": "exa",
    "include_links": true,
    "include_images": true
  }'
```

---

## ğŸ¤ Contribution

Les contributions sont les bienvenues! Voici comment participer:

1. ğŸ´ **Fork** le dÃ©pÃ´t
2. ğŸ”„ CrÃ©ez une **branche** (`git checkout -b feature/ma-fonctionnalite`)
3. âœï¸ Faites vos **modifications**
4. ğŸ“¦ **Commit** vos changements (`git commit -m 'Ajout de ma fonctionnalitÃ©'`)
5. ğŸ“¤ **Push** vers la branche (`git push origin feature/ma-fonctionnalite`)
6. ğŸ” Ouvrez une **Pull Request**

### ğŸ’¼ IdÃ©es de contributions

- ğŸ”Œ Support de nouveaux moteurs d'extraction
- ğŸ§° Options d'extraction additionnelles
- ğŸ“Š Visualisation du contenu extrait
- ğŸŒ Interface utilisateur web simple
- ğŸ“ Expansion de la documentation
- ğŸ§ª Tests additionnels

---

## ğŸ“œ Licence

Ce projet est sous licence [MIT](LICENSE) - voir le fichier LICENSE pour plus de dÃ©tails.

---

## â“ FAQ

### ğŸ”„ Quelle est la diffÃ©rence entre les moteurs d'extraction?
- **Exa AI**: Meilleur pour l'extraction structurÃ©e et les mÃ©tadonnÃ©es, avec support des sous-pages
- **Tavily**: Performances supÃ©rieures pour la rÃ©cupÃ©ration de contenu brut et l'extraction profonde

### ğŸ’° Quel moteur est le plus Ã©conomique?
Les coÃ»ts varient selon l'utilisation, mais les deux services offrent des plans gratuits gÃ©nÃ©reux pour commencer.

### ğŸš€ Comment optimiser les performances?
Activez la mise en cache, limitez l'extraction aux donnÃ©es nÃ©cessaires (dÃ©sactivez les images si non requises), et utilisez la profondeur d'extraction appropriÃ©e.

### ğŸŒ L'API fonctionne-t-elle sur tous les sites web?
La plupart des sites modernes sont supportÃ©s, mais certains peuvent avoir des protections anti-scraping ou des structures complexes qui rendent l'extraction difficile.

### âš™ï¸ Comment adapter l'API Ã  mes besoins spÃ©cifiques?
Le code source est conÃ§u pour Ãªtre facilement extensible - vous pouvez ajouter de nouveaux moteurs d'extraction ou personnaliser les paramÃ¨tres existants.

---

## ğŸ‘¨â€ğŸ’» Auteurs

- ğŸš€ [Simon-Pierre Boucher](https://github.com/simonpierreboucher0) - CrÃ©ateur principal

---

<p align="center">
â­ N'oubliez pas de mettre une Ã©toile si ce projet vous a Ã©tÃ© utile! â­
</p>

<p align="center">
ğŸ”— <a href="https://github.com/simonpierreboucher0/web-content-api">GitHub</a> | 
ğŸ› <a href="https://github.com/simonpierreboucher0/web-content-api/issues">Signaler un problÃ¨me</a> | 
ğŸ’¡ <a href="https://github.com/simonpierreboucher0/web-content-api/discussions">Discussions</a>
</p>
